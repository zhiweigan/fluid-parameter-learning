{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "import FluidEnv\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fcb86717fb0>"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "learning_rate = 0.0005\n",
    "gamma = 0.99\n",
    "num_params = 4\n",
    "max_change = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.state_space = num_params\n",
    "        self.action_space = num_params\n",
    "        \n",
    "        self.l1 = nn.Linear(self.state_space, 128, bias=False)\n",
    "        self.l2 = nn.Linear(128, self.action_space, bias=False)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Episode policy and reward history \n",
    "        self.policy_history = Variable(torch.Tensor()) \n",
    "        self.reward_episode = []\n",
    "        # Overall reward and loss history\n",
    "        self.reward_history = []\n",
    "        self.loss_history = []\n",
    "\n",
    "    def forward(self, x):            \n",
    "        model = torch.nn.Sequential(\n",
    "            self.l1,\n",
    "            nn.Dropout(p=0.6),\n",
    "            nn.ReLU(),\n",
    "            self.l2,\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        return model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    #Select an action (0 or 1) by running policy model and choosing based on the probabilities in state\n",
    "    action_space = policy(Variable(state))\n",
    "    \n",
    "    means, sigs = action_space, torch.exp(torch.Tensor(np.zeros(num_params) + 0.1))\n",
    "    \n",
    "    dists = torch.distributions.Normal(means, sigs)\n",
    "    samples = dists.sample()\n",
    "    \n",
    "    # Add log probability of our chosen action to our history    \n",
    "    policy.policy_history = torch.cat((policy.policy_history, torch.prod(dists.log_prob(samples)).reshape(1)))\n",
    "        \n",
    "    return samples * max_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy():\n",
    "    R = 0\n",
    "    rewards = []\n",
    "    \n",
    "    # Discount future rewards back to the present using gamma\n",
    "    for r in policy.reward_episode[::-1]:\n",
    "        R = r + policy.gamma * R\n",
    "        rewards.insert(0,R)\n",
    "        \n",
    "    # Scale rewards\n",
    "    rewards = torch.FloatTensor(rewards)\n",
    "    rewards = (rewards - rewards.mean()) / (rewards.std() + np.finfo(np.float32).eps)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = (torch.sum(torch.mul(policy.policy_history, Variable(rewards)).mul(-1), -1))\n",
    "    \n",
    "    # Update network weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    #Save and intialize episode history counters\n",
    "    policy.loss_history.append(loss.data)\n",
    "    policy.reward_history.append(np.sum(policy.reward_episode))\n",
    "    policy.policy_history = Variable(torch.Tensor([]))\n",
    "    policy.reward_episode= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(FluidEnv)\n",
    "\n",
    "env = FluidEnv.FluidEnv(4, './data/sound.wav', 20)\n",
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)\n",
    "\n",
    "def main(episodes):\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset() # Reset environment and record the starting state\n",
    "        done = False       \n",
    "    \n",
    "        while not done:\n",
    "            action = select_action(state)            \n",
    "            # Step through environment using chosen action\n",
    "            state, reward, done, _ = env.step(action.data)\n",
    "\n",
    "            # Save reward\n",
    "            policy.reward_episode.append(reward)\n",
    "        \n",
    "        update_policy()\n",
    "\n",
    "        if episode % 1 == 0:\n",
    "            print('Episode {}\\tReward: {}\\t State: {}'.format(episode, reward, state.tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'main' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-383a53fa64a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepisodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'main' is not defined"
     ]
    }
   ],
   "source": [
    "episodes = 500\n",
    "main(episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
