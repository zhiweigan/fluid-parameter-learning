{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import namedtuple\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.distributions import Categorical\n",
    "import random\n",
    "import math\n",
    "import FluidEnv\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fcbd0e5af90>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "learning_rate = 0.00005\n",
    "gamma = 1\n",
    "num_params = 6\n",
    "change = 0.05\n",
    "is_ipython = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.lin1 = nn.Linear(outputs // 2, 32)\n",
    "        self.lin2 = nn.Linear(32, 64)\n",
    "        self.lin3 = nn.Linear(64, 64)\n",
    "        self.lin4 = nn.Linear(64, 64)\n",
    "        self.lin5 = nn.Linear(64, 32)\n",
    "        self.head = nn.Linear(32, outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = F.relu(self.lin3(x))\n",
    "        x = F.relu(self.lin4(x))\n",
    "        x = F.relu(self.lin5(x))\n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "GAMMA = 1\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 2\n",
    "\n",
    "# Get number of actions from gym action space\n",
    "n_actions = num_params * 2\n",
    "\n",
    "policy_net = DQN(n_actions).to(device)\n",
    "target_net = DQN(n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(200)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "actions_add = [\n",
    "    [change if j == i else 0 for j in range(num_params)] for i in range(num_params)\n",
    "]\n",
    "actions_sub = [\n",
    "    [-change if j == i else 0 for j in range(num_params)] for i in range(num_params)\n",
    "]\n",
    "actions = actions_add + actions_sub\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return torch.tensor([[policy_net(state).argmax()]], device=device, dtype=torch.long)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    \n",
    "    non_final_next_states = torch.cat([torch.Tensor(s) for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    \n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)    \n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tReward: tensor([-75.6399])\t State: [1.0, 0.6499999761581421, 0.4000000059604645, 0.550000011920929, 0.4000000059604645, 0.20000000298023224]\n",
      "Episode 0\t Max Reward: -41.174991607666016\t Max State: [1.0, 0.6499999761581421, 0.3499999940395355, 0.550000011920929, 0.4000000059604645, 0.25]\n",
      "Episode 1\tReward: tensor([-1452.1062])\t State: [0.75, 0.44999998807907104, 0.6499999761581421, 0.4000000059604645, 0.550000011920929, 0.0]\n",
      "Episode 2\tReward: tensor([-8.6203])\t State: [0.30000001192092896, 0.550000011920929, 0.5, 0.6000000238418579, 1.0, 0.44999998807907104]\n",
      "Episode 3\tReward: tensor([-8.3850])\t State: [0.25, 0.6499999761581421, 0.44999998807907104, 0.5, 0.949999988079071, 0.5]\n",
      "Episode 4\tReward: tensor([-14.7798])\t State: [0.5, 0.5, 0.5, 0.550000011920929, 1.0, 0.550000011920929]\n",
      "Episode 5\tReward: tensor([-16.8419])\t State: [0.550000011920929, 0.5, 0.44999998807907104, 0.44999998807907104, 1.0, 1.0]\n",
      "Episode 6\tReward: tensor([-5.6689])\t State: [0.25, 0.44999998807907104, 0.550000011920929, 0.6000000238418579, 1.0, 0.6000000238418579]\n",
      "Episode 7\tReward: tensor([-17.1865])\t State: [0.6499999761581421, 0.30000001192092896, 0.5, 0.20000000298023224, 1.0, 0.44999998807907104]\n",
      "Episode 8\tReward: tensor([-14.2766])\t State: [0.5, 0.3499999940395355, 0.6000000238418579, 0.0, 1.0, 0.5]\n",
      "Episode 9\tReward: tensor([-5.1608])\t State: [0.05000000074505806, 0.550000011920929, 0.5, 0.4000000059604645, 0.949999988079071, 0.5]\n",
      "Episode 10\tReward: tensor([-9.2058])\t State: [0.3499999940395355, 0.5, 0.5, 0.5, 1.0, 0.5]\n",
      "Episode 10\t Max Reward: -5.160794258117676\t Max State: [0.05000000074505806, 0.550000011920929, 0.5, 0.4000000059604645, 0.949999988079071, 0.5]\n",
      "Episode 11\tReward: tensor([-11.2880])\t State: [0.44999998807907104, 0.3499999940395355, 0.4000000059604645, 0.0, 1.0, 0.550000011920929]\n",
      "Episode 12\tReward: tensor([-11.2532])\t State: [0.44999998807907104, 0.3499999940395355, 0.44999998807907104, 0.0, 1.0, 0.6000000238418579]\n",
      "Episode 13\tReward: tensor([-21.4546])\t State: [0.550000011920929, 0.800000011920929, 0.25, 0.0, 1.0, 0.6000000238418579]\n",
      "Episode 14\tReward: tensor([-11.8046])\t State: [0.44999998807907104, 0.15000000596046448, 0.25, 0.0, 1.0, 0.44999998807907104]\n",
      "Episode 15\tReward: tensor([-16.1776])\t State: [0.30000001192092896, 0.30000001192092896, 0.5, 0.44999998807907104, 1.0, 0.15000000596046448]\n",
      "Episode 16\tReward: tensor([-8.5614])\t State: [0.25, 0.5, 0.6499999761581421, 0.44999998807907104, 0.949999988079071, 0.30000001192092896]\n",
      "Episode 17\tReward: tensor([-9.6118])\t State: [0.30000001192092896, 0.550000011920929, 0.6000000238418579, 0.550000011920929, 0.8999999761581421, 0.5]\n",
      "Episode 18\tReward: tensor([-2.2319])\t State: [0.15000000596046448, 0.4000000059604645, 0.550000011920929, 0.5, 0.8500000238418579, 0.5]\n",
      "Episode 19\tReward: tensor([-2.3707])\t State: [0.0, 0.25, 0.5, 0.5, 0.8500000238418579, 0.30000001192092896]\n",
      "Episode 20\tReward: tensor([-1.5190])\t State: [0.0, 0.30000001192092896, 0.5, 0.5, 0.800000011920929, 0.6000000238418579]\n",
      "Episode 20\t Max Reward: -1.0333079099655151\t Max State: [0.0, 0.3499999940395355, 0.5, 0.5, 0.75, 0.6000000238418579]\n",
      "Episode 21\tReward: tensor([-1.4390])\t State: [0.0, 0.4000000059604645, 0.44999998807907104, 0.6000000238418579, 0.800000011920929, 0.5]\n",
      "Episode 22\tReward: tensor([-1.6712])\t State: [0.15000000596046448, 0.0, 0.44999998807907104, 0.44999998807907104, 0.75, 0.4000000059604645]\n",
      "Episode 23\tReward: tensor([-86.2730])\t State: [0.0, 0.15000000596046448, 0.4000000059604645, 0.44999998807907104, 1.0, 0.10000000149011612]\n",
      "Episode 24\tReward: tensor([-1.0327])\t State: [0.0, 0.3499999940395355, 0.5, 0.5, 0.75, 0.5]\n",
      "Episode 25\tReward: tensor([-1.6212])\t State: [0.0, 0.0, 0.5, 0.550000011920929, 0.75, 0.5]\n",
      "Episode 26\tReward: tensor([-4.2452])\t State: [0.0, 0.20000000298023224, 0.6000000238418579, 0.44999998807907104, 0.699999988079071, 0.550000011920929]\n",
      "Episode 27\tReward: tensor([-13.7458])\t State: [0.0, 0.15000000596046448, 0.800000011920929, 0.6000000238418579, 0.8999999761581421, 0.3499999940395355]\n",
      "Episode 28\tReward: tensor([-2.8661])\t State: [0.05000000074505806, 0.3499999940395355, 0.550000011920929, 0.550000011920929, 0.699999988079071, 0.75]\n",
      "Episode 29\tReward: tensor([-6.8726])\t State: [0.05000000074505806, 0.0, 0.3499999940395355, 0.550000011920929, 0.6499999761581421, 0.25]\n",
      "Episode 30\tReward: tensor([-4.4556])\t State: [0.0, 0.0, 0.25, 0.75, 0.699999988079071, 0.4000000059604645]\n",
      "Episode 30\t Max Reward: -0.4236515760421753\t Max State: [0.20000000298023224, 0.3499999940395355, 0.44999998807907104, 0.5, 0.699999988079071, 0.44999998807907104]\n",
      "Episode 31\tReward: tensor([-41.6631])\t State: [0.3499999940395355, 0.0, 0.3499999940395355, 0.5, 0.550000011920929, 0.75]\n",
      "Episode 32\tReward: tensor([-9.3922])\t State: [0.15000000596046448, 0.20000000298023224, 0.550000011920929, 0.699999988079071, 0.6499999761581421, 1.0]\n",
      "Episode 33\tReward: tensor([-6.8545])\t State: [0.0, 0.4000000059604645, 0.05000000074505806, 0.550000011920929, 0.6000000238418579, 0.5]\n",
      "Episode 34\tReward: tensor([-2.3316])\t State: [0.0, 0.44999998807907104, 0.3499999940395355, 0.3499999940395355, 0.6499999761581421, 0.3499999940395355]\n",
      "Episode 35\tReward: tensor([-3.0829])\t State: [0.0, 0.5, 0.30000001192092896, 0.4000000059604645, 0.6499999761581421, 0.550000011920929]\n",
      "Episode 36\tReward: tensor([-105.5972])\t State: [0.4000000059604645, 0.20000000298023224, 0.0, 0.5, 0.15000000596046448, 0.8999999761581421]\n",
      "Episode 37\tReward: tensor([-1.6024])\t State: [0.0, 0.0, 0.44999998807907104, 0.44999998807907104, 0.75, 0.30000001192092896]\n",
      "Episode 38\tReward: tensor([-4.6592])\t State: [0.0, 0.44999998807907104, 0.44999998807907104, 0.8999999761581421, 0.75, 0.44999998807907104]\n",
      "Episode 39\tReward: tensor([-2.9150])\t State: [0.0, 0.30000001192092896, 0.5, 0.75, 0.75, 0.550000011920929]\n",
      "Episode 40\tReward: tensor([-4.2094])\t State: [0.0, 0.15000000596046448, 0.44999998807907104, 0.8500000238418579, 0.75, 0.550000011920929]\n",
      "Episode 40\t Max Reward: -0.4236515760421753\t Max State: [0.20000000298023224, 0.3499999940395355, 0.44999998807907104, 0.5, 0.699999988079071, 0.44999998807907104]\n",
      "Episode 41\tReward: tensor([-10.4228])\t State: [0.0, 0.20000000298023224, 0.550000011920929, 1.0, 0.75, 0.699999988079071]\n",
      "Episode 42\tReward: tensor([-30.3434])\t State: [0.0, 0.05000000074505806, 0.44999998807907104, 1.0, 0.6000000238418579, 0.699999988079071]\n",
      "Episode 43\tReward: tensor([-10.4575])\t State: [0.0, 0.05000000074505806, 0.4000000059604645, 1.0, 0.6499999761581421, 0.699999988079071]\n",
      "Episode 44\tReward: tensor([-8.4810])\t State: [0.0, 0.25, 0.6000000238418579, 0.75, 0.699999988079071, 0.550000011920929]\n",
      "Episode 45\tReward: tensor([-3.6784])\t State: [0.0, 0.20000000298023224, 0.550000011920929, 0.6000000238418579, 0.699999988079071, 0.5]\n",
      "Episode 46\tReward: tensor([-19.1828])\t State: [0.0, 0.3499999940395355, 0.75, 0.8500000238418579, 0.75, 0.550000011920929]\n",
      "Episode 47\tReward: tensor([-5.7783])\t State: [0.0, 0.15000000596046448, 0.5, 0.800000011920929, 0.699999988079071, 0.44999998807907104]\n",
      "Episode 48\tReward: tensor([-3.4463])\t State: [0.0, 0.3499999940395355, 0.30000001192092896, 0.30000001192092896, 0.6000000238418579, 0.44999998807907104]\n",
      "Episode 49\tReward: tensor([-4.1101])\t State: [0.0, 0.15000000596046448, 0.3499999940395355, 0.8500000238418579, 0.699999988079071, 0.550000011920929]\n",
      "Episode 50\tReward: tensor([-4.0294])\t State: [0.0, 0.0, 0.5, 0.25, 0.6499999761581421, 0.44999998807907104]\n",
      "Episode 50\t Max Reward: -0.27109068632125854\t Max State: [0.20000000298023224, 0.30000001192092896, 0.44999998807907104, 0.550000011920929, 0.75, 0.5]\n",
      "Episode 51\tReward: tensor([-17.1257])\t State: [0.0, 0.44999998807907104, 0.20000000298023224, 0.6499999761581421, 0.6499999761581421, 0.25]\n",
      "Episode 52\tReward: tensor([-6.5573])\t State: [0.05000000074505806, 0.30000001192092896, 0.15000000596046448, 0.44999998807907104, 0.550000011920929, 0.550000011920929]\n",
      "Episode 53\tReward: tensor([-5.6106])\t State: [0.0, 0.5, 0.15000000596046448, 0.6499999761581421, 0.8500000238418579, 0.5]\n",
      "Episode 54\tReward: tensor([-78.3927])\t State: [0.30000001192092896, 0.10000000149011612, 0.550000011920929, 0.44999998807907104, 0.550000011920929, 0.5]\n",
      "Episode 55\tReward: tensor([-4.6512])\t State: [0.20000000298023224, 0.15000000596046448, 0.3499999940395355, 0.949999988079071, 0.8999999761581421, 0.6000000238418579]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 56\tReward: tensor([-3.4730])\t State: [0.0, 0.4000000059604645, 0.20000000298023224, 0.6000000238418579, 0.6000000238418579, 0.550000011920929]\n",
      "Episode 57\tReward: tensor([-3.7665])\t State: [0.0, 0.3499999940395355, 0.3499999940395355, 0.800000011920929, 0.8999999761581421, 0.44999998807907104]\n",
      "Episode 58\tReward: tensor([-18.7416])\t State: [0.0, 0.4000000059604645, 0.0, 0.699999988079071, 0.550000011920929, 0.30000001192092896]\n",
      "Episode 59\tReward: tensor([-43.2105])\t State: [0.0, 0.6000000238418579, 0.05000000074505806, 0.550000011920929, 0.550000011920929, 0.800000011920929]\n",
      "Episode 60\tReward: tensor([-6.0335])\t State: [0.0, 0.0, 0.20000000298023224, 0.8999999761581421, 0.699999988079071, 0.6000000238418579]\n",
      "Episode 60\t Max Reward: -0.27109068632125854\t Max State: [0.20000000298023224, 0.30000001192092896, 0.44999998807907104, 0.550000011920929, 0.75, 0.5]\n",
      "Episode 61\tReward: tensor([-1.9700])\t State: [0.0, 0.0, 0.30000001192092896, 0.5, 0.6499999761581421, 0.44999998807907104]\n",
      "Episode 62\tReward: tensor([-4.4874])\t State: [0.30000001192092896, 0.30000001192092896, 0.25, 0.25, 0.699999988079071, 0.6000000238418579]\n",
      "Episode 63\tReward: tensor([-15.9102])\t State: [0.0, 0.30000001192092896, 0.5, 0.6000000238418579, 0.6000000238418579, 1.0]\n",
      "Episode 64\tReward: tensor([-13.0646])\t State: [0.20000000298023224, 0.05000000074505806, 0.0, 0.6000000238418579, 0.6000000238418579, 0.25]\n",
      "Episode 65\tReward: tensor([-13.5397])\t State: [0.44999998807907104, 0.0, 0.05000000074505806, 0.3499999940395355, 0.800000011920929, 0.699999988079071]\n",
      "Episode 66\tReward: tensor([-5.4050])\t State: [0.0, 0.44999998807907104, 0.15000000596046448, 0.800000011920929, 0.800000011920929, 0.5]\n",
      "Episode 67\tReward: tensor([-22.4542])\t State: [0.30000001192092896, 0.699999988079071, 0.0, 0.5, 0.699999988079071, 0.20000000298023224]\n",
      "Episode 68\tReward: tensor([-7.8989])\t State: [0.0, 0.4000000059604645, 0.05000000074505806, 0.6000000238418579, 0.6000000238418579, 0.800000011920929]\n",
      "Episode 69\tReward: tensor([-6.5087])\t State: [0.0, 0.20000000298023224, 0.15000000596046448, 0.8500000238418579, 0.6499999761581421, 0.699999988079071]\n",
      "Episode 70\tReward: tensor([-7.5236])\t State: [0.20000000298023224, 0.800000011920929, 0.10000000149011612, 0.800000011920929, 0.8999999761581421, 1.0]\n",
      "Episode 70\t Max Reward: -0.27109068632125854\t Max State: [0.20000000298023224, 0.30000001192092896, 0.44999998807907104, 0.550000011920929, 0.75, 0.5]\n",
      "Episode 71\tReward: tensor([-15.3889])\t State: [0.0, 0.15000000596046448, 0.0, 0.550000011920929, 0.550000011920929, 0.5]\n",
      "Episode 72\tReward: tensor([-14.6615])\t State: [0.5, 0.5, 0.0, 0.8500000238418579, 0.8999999761581421, 0.4000000059604645]\n",
      "Episode 73\tReward: tensor([-10.0737])\t State: [0.0, 0.25, 0.05000000074505806, 0.699999988079071, 0.550000011920929, 0.8500000238418579]\n",
      "Episode 74\tReward: tensor([-3.0041])\t State: [0.10000000149011612, 0.10000000149011612, 0.44999998807907104, 0.6499999761581421, 0.8500000238418579, 0.800000011920929]\n",
      "Episode 75\tReward: tensor([-14.6359])\t State: [0.550000011920929, 0.0, 0.6000000238418579, 0.6499999761581421, 0.949999988079071, 0.6000000238418579]\n",
      "Episode 76\tReward: tensor([-5.7510])\t State: [0.10000000149011612, 0.30000001192092896, 0.20000000298023224, 0.75, 0.800000011920929, 1.0]\n",
      "Episode 77\tReward: tensor([-7.6007])\t State: [0.25, 0.6000000238418579, 0.25, 1.0, 1.0, 0.3499999940395355]\n",
      "Episode 78\tReward: tensor([-3.8267])\t State: [0.15000000596046448, 0.05000000074505806, 0.550000011920929, 0.550000011920929, 1.0, 0.699999988079071]\n",
      "Episode 79\tReward: tensor([-15.1331])\t State: [0.0, 0.8500000238418579, 0.5, 0.550000011920929, 0.75, 0.3499999940395355]\n",
      "Episode 80\tReward: tensor([-8.1813])\t State: [0.0, 0.550000011920929, 0.5, 1.0, 0.8999999761581421, 0.699999988079071]\n",
      "Episode 80\t Max Reward: -0.27109068632125854\t Max State: [0.20000000298023224, 0.30000001192092896, 0.44999998807907104, 0.550000011920929, 0.75, 0.5]\n",
      "Episode 81\tReward: tensor([-12.1115])\t State: [0.5, 0.05000000074505806, 0.550000011920929, 0.6499999761581421, 1.0, 0.5]\n",
      "Episode 82\tReward: tensor([-3.2374])\t State: [0.15000000596046448, 0.550000011920929, 0.4000000059604645, 0.6000000238418579, 0.8500000238418579, 0.6000000238418579]\n",
      "Episode 83\tReward: tensor([-5.7991])\t State: [0.0, 0.6000000238418579, 0.4000000059604645, 0.4000000059604645, 1.0, 1.0]\n",
      "Episode 84\tReward: tensor([-10.2833])\t State: [0.3499999940395355, 0.550000011920929, 0.44999998807907104, 0.10000000149011612, 0.949999988079071, 0.550000011920929]\n",
      "Episode 85\tReward: tensor([-10.1955])\t State: [0.25, 0.44999998807907104, 0.0, 0.6499999761581421, 1.0, 0.949999988079071]\n",
      "Episode 86\tReward: tensor([-17.4200])\t State: [0.6499999761581421, 0.8999999761581421, 0.5, 0.4000000059604645, 1.0, 0.550000011920929]\n",
      "Episode 87\tReward: tensor([-20.5243])\t State: [0.25, 0.8999999761581421, 0.5, 0.6499999761581421, 1.0, 1.0]\n",
      "Episode 88\tReward: tensor([-7.5395])\t State: [0.20000000298023224, 0.5, 0.550000011920929, 0.8500000238418579, 1.0, 0.6499999761581421]\n",
      "Episode 89\tReward: tensor([-7.7634])\t State: [0.0, 0.6499999761581421, 0.550000011920929, 0.6499999761581421, 0.800000011920929, 0.6000000238418579]\n",
      "Episode 90\tReward: tensor([-8.1308])\t State: [0.44999998807907104, 0.05000000074505806, 0.4000000059604645, 0.3499999940395355, 0.75, 0.4000000059604645]\n",
      "Episode 90\t Max Reward: -0.27109068632125854\t Max State: [0.20000000298023224, 0.30000001192092896, 0.44999998807907104, 0.550000011920929, 0.75, 0.5]\n",
      "Episode 91\tReward: tensor([-30.6119])\t State: [0.6000000238418579, 0.5, 0.44999998807907104, 1.0, 1.0, 0.5]\n",
      "Episode 92\tReward: tensor([-9.2731])\t State: [0.15000000596046448, 0.5, 0.0, 0.949999988079071, 1.0, 0.44999998807907104]\n",
      "Episode 93\tReward: tensor([-25.1111])\t State: [0.0, 0.4000000059604645, 0.30000001192092896, 0.550000011920929, 0.550000011920929, 0.44999998807907104]\n",
      "Episode 94\tReward: tensor([-42.4650])\t State: [0.25, 0.44999998807907104, 1.0, 0.6499999761581421, 1.0, 0.44999998807907104]\n",
      "Episode 95\tReward: tensor([-6.2927])\t State: [0.20000000298023224, 0.6499999761581421, 0.44999998807907104, 0.6499999761581421, 0.75, 0.44999998807907104]\n",
      "Episode 96\tReward: tensor([-6.4909])\t State: [0.0, 0.10000000149011612, 0.699999988079071, 0.5, 0.75, 0.44999998807907104]\n",
      "Episode 97\tReward: tensor([-4.4383])\t State: [0.0, 0.4000000059604645, 0.30000001192092896, 0.6000000238418579, 1.0, 1.0]\n",
      "Episode 98\tReward: tensor([-33.3493])\t State: [0.30000001192092896, 0.4000000059604645, 0.949999988079071, 0.6000000238418579, 1.0, 0.8999999761581421]\n",
      "Episode 99\tReward: tensor([-11.1386])\t State: [0.3499999940395355, 0.0, 0.10000000149011612, 0.5, 1.0, 1.0]\n",
      "Episode 100\tReward: tensor([-1.6852])\t State: [0.25, 0.0, 0.550000011920929, 0.5, 0.800000011920929, 0.3499999940395355]\n",
      "Episode 100\t Max Reward: -0.27109068632125854\t Max State: [0.20000000298023224, 0.30000001192092896, 0.44999998807907104, 0.550000011920929, 0.75, 0.5]\n",
      "Episode 101\tReward: tensor([-3.6643])\t State: [0.25, 0.05000000074505806, 0.5, 0.800000011920929, 0.8999999761581421, 0.6499999761581421]\n",
      "Episode 102\tReward: tensor([-2.0313])\t State: [0.20000000298023224, 0.0, 0.44999998807907104, 0.550000011920929, 0.8500000238418579, 0.6499999761581421]\n",
      "Episode 103\tReward: tensor([-4.3959])\t State: [0.20000000298023224, 0.15000000596046448, 0.550000011920929, 0.75, 1.0, 0.699999988079071]\n",
      "Episode 104\tReward: tensor([-6.5804])\t State: [0.0, 0.4000000059604645, 0.5, 0.949999988079071, 0.75, 0.5]\n",
      "Episode 105\tReward: tensor([-5.9326])\t State: [0.25, 0.4000000059604645, 0.44999998807907104, 1.0, 0.800000011920929, 0.550000011920929]\n",
      "Episode 106\tReward: tensor([-9.6212])\t State: [0.44999998807907104, 0.05000000074505806, 0.44999998807907104, 0.6499999761581421, 0.699999988079071, 0.550000011920929]\n",
      "Episode 107\tReward: tensor([-3.7038])\t State: [0.15000000596046448, 0.30000001192092896, 0.550000011920929, 0.75, 0.8999999761581421, 0.8999999761581421]\n",
      "Episode 108\tReward: tensor([-202.5733])\t State: [0.15000000596046448, 0.10000000149011612, 0.4000000059604645, 1.0, 1.0, 0.15000000596046448]\n",
      "Episode 109\tReward: tensor([-10.7177])\t State: [0.15000000596046448, 0.44999998807907104, 0.75, 0.6000000238418579, 0.8500000238418579, 0.6499999761581421]\n",
      "Episode 110\tReward: tensor([-8.6560])\t State: [0.0, 0.0, 0.699999988079071, 0.6499999761581421, 1.0, 0.3499999940395355]\n",
      "Episode 110\t Max Reward: -0.27109068632125854\t Max State: [0.20000000298023224, 0.30000001192092896, 0.44999998807907104, 0.550000011920929, 0.75, 0.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 111\tReward: tensor([-26.8582])\t State: [0.5, 0.0, 0.8999999761581421, 0.699999988079071, 0.8999999761581421, 0.6000000238418579]\n",
      "Episode 112\tReward: tensor([-2.1681])\t State: [0.15000000596046448, 0.0, 0.550000011920929, 0.550000011920929, 0.75, 0.8500000238418579]\n",
      "Episode 113\tReward: tensor([-6.9667])\t State: [0.4000000059604645, 0.10000000149011612, 0.44999998807907104, 0.75, 0.800000011920929, 1.0]\n",
      "Episode 114\tReward: tensor([-2.4960])\t State: [0.0, 0.10000000149011612, 0.550000011920929, 0.6000000238418579, 0.800000011920929, 0.75]\n",
      "Episode 115\tReward: tensor([-8.2919])\t State: [0.0, 0.4000000059604645, 0.6499999761581421, 0.800000011920929, 0.8999999761581421, 1.0]\n",
      "Episode 116\tReward: tensor([-3.1561])\t State: [0.0, 0.10000000149011612, 0.44999998807907104, 0.699999988079071, 0.8500000238418579, 0.6499999761581421]\n",
      "Episode 117\tReward: tensor([-1.9109])\t State: [0.0, 0.3499999940395355, 0.44999998807907104, 0.6499999761581421, 0.699999988079071, 0.4000000059604645]\n",
      "Episode 118\tReward: tensor([-1.8160])\t State: [0.0, 0.3499999940395355, 0.550000011920929, 0.550000011920929, 0.75, 0.5]\n",
      "Episode 119\tReward: tensor([-5.9207])\t State: [0.10000000149011612, 0.3499999940395355, 0.550000011920929, 0.8500000238418579, 0.75, 0.44999998807907104]\n",
      "Episode 120\tReward: tensor([-4.8661])\t State: [0.05000000074505806, 0.44999998807907104, 0.6000000238418579, 0.6499999761581421, 0.75, 0.699999988079071]\n",
      "Episode 120\t Max Reward: -0.27109068632125854\t Max State: [0.20000000298023224, 0.30000001192092896, 0.44999998807907104, 0.550000011920929, 0.75, 0.5]\n",
      "Episode 121\tReward: tensor([-8.6426])\t State: [0.05000000074505806, 0.5, 0.6000000238418579, 0.6499999761581421, 0.699999988079071, 0.4000000059604645]\n",
      "Episode 122\tReward: tensor([-1.6323])\t State: [0.10000000149011612, 0.0, 0.5, 0.5, 0.75, 0.44999998807907104]\n",
      "Episode 123\tReward: tensor([-16.2231])\t State: [0.20000000298023224, 0.6499999761581421, 0.6000000238418579, 1.0, 0.8500000238418579, 0.800000011920929]\n",
      "Episode 124\tReward: tensor([-2.9538])\t State: [0.10000000149011612, 0.4000000059604645, 0.44999998807907104, 0.5, 0.6499999761581421, 0.550000011920929]\n",
      "Episode 125\tReward: tensor([-3.3573])\t State: [0.0, 0.30000001192092896, 0.5, 0.550000011920929, 1.0, 0.25]\n",
      "Episode 126\tReward: tensor([-3.8476])\t State: [0.10000000149011612, 0.550000011920929, 0.44999998807907104, 0.75, 0.800000011920929, 0.5]\n",
      "Episode 127\tReward: tensor([-1.9728])\t State: [0.05000000074505806, 0.30000001192092896, 0.5, 0.6499999761581421, 0.75, 0.4000000059604645]\n",
      "Episode 128\tReward: tensor([-474.3086])\t State: [0.550000011920929, 0.5, 0.44999998807907104, 0.5, 0.0, 0.5]\n",
      "Episode 129\tReward: tensor([-5.9894])\t State: [0.3499999940395355, 0.4000000059604645, 0.5, 0.550000011920929, 0.800000011920929, 0.949999988079071]\n",
      "Episode 130\tReward: tensor([-3.7339])\t State: [0.30000001192092896, 0.3499999940395355, 0.44999998807907104, 0.3499999940395355, 0.800000011920929, 0.6000000238418579]\n",
      "Episode 130\t Max Reward: -0.27109068632125854\t Max State: [0.20000000298023224, 0.30000001192092896, 0.44999998807907104, 0.550000011920929, 0.75, 0.5]\n",
      "Episode 131\tReward: tensor([-26.0230])\t State: [0.75, 0.5, 0.5, 1.0, 0.8999999761581421, 1.0]\n",
      "Episode 132\tReward: tensor([-2.2760])\t State: [0.0, 0.25, 0.550000011920929, 0.4000000059604645, 0.800000011920929, 0.8500000238418579]\n",
      "Episode 133\tReward: tensor([-17.1683])\t State: [0.0, 0.15000000596046448, 0.8500000238418579, 0.4000000059604645, 0.949999988079071, 0.5]\n",
      "Episode 134\tReward: tensor([-3.6278])\t State: [0.0, 0.20000000298023224, 0.6000000238418579, 0.3499999940395355, 0.8500000238418579, 0.550000011920929]\n",
      "Episode 135\tReward: tensor([-23.6941])\t State: [0.0, 0.4000000059604645, 0.8999999761581421, 0.4000000059604645, 1.0, 0.699999988079071]\n",
      "Episode 136\tReward: tensor([-14.0802])\t State: [0.10000000149011612, 0.6499999761581421, 0.699999988079071, 0.550000011920929, 0.949999988079071, 0.8999999761581421]\n",
      "Episode 137\tReward: tensor([-4.6217])\t State: [0.25, 0.550000011920929, 0.4000000059604645, 0.550000011920929, 0.8500000238418579, 0.6499999761581421]\n",
      "Episode 138\tReward: tensor([-165.8544])\t State: [0.44999998807907104, 0.0, 0.5, 0.6000000238418579, 0.3499999940395355, 1.0]\n",
      "Episode 139\tReward: tensor([-30.7030])\t State: [0.10000000149011612, 1.0, 0.6499999761581421, 0.75, 1.0, 0.550000011920929]\n",
      "Episode 140\tReward: tensor([-2.6606])\t State: [0.15000000596046448, 0.3499999940395355, 0.5, 0.550000011920929, 0.75, 0.25]\n",
      "Episode 140\t Max Reward: -5.650676030199975e-06\t Max State: [0.20000000298023224, 0.30000001192092896, 0.4000000059604645, 0.5, 0.699999988079071, 0.6000000238418579]\n",
      "Episode 141\tReward: tensor([-1.3317])\t State: [0.20000000298023224, 0.0, 0.4000000059604645, 0.6499999761581421, 0.699999988079071, 0.699999988079071]\n",
      "Episode 142\tReward: tensor([-9.6972])\t State: [0.25, 0.699999988079071, 0.5, 0.3499999940395355, 0.800000011920929, 0.550000011920929]\n",
      "Episode 143\tReward: tensor([-5.7467])\t State: [0.05000000074505806, 0.550000011920929, 0.6000000238418579, 0.5, 0.8500000238418579, 0.6000000238418579]\n",
      "Episode 144\tReward: tensor([-7.7449])\t State: [0.25, 0.44999998807907104, 0.5, 0.25, 0.6499999761581421, 0.3499999940395355]\n",
      "Episode 145\tReward: tensor([-15.4966])\t State: [0.25, 0.75, 0.6000000238418579, 0.20000000298023224, 0.75, 0.20000000298023224]\n",
      "Episode 146\tReward: tensor([-3.2361])\t State: [0.05000000074505806, 0.5, 0.3499999940395355, 0.800000011920929, 0.699999988079071, 0.6499999761581421]\n",
      "Episode 147\tReward: tensor([-5.6238])\t State: [0.0, 0.5, 0.550000011920929, 0.75, 0.75, 0.44999998807907104]\n",
      "Episode 148\tReward: tensor([-10.8185])\t State: [0.0, 0.5, 0.75, 0.5, 0.8500000238418579, 0.44999998807907104]\n",
      "Episode 149\tReward: tensor([-16.2471])\t State: [0.4000000059604645, 0.30000001192092896, 0.800000011920929, 0.550000011920929, 0.800000011920929, 0.6000000238418579]\n",
      "Episode 150\tReward: tensor([-9.9864])\t State: [0.30000001192092896, 0.44999998807907104, 0.699999988079071, 0.4000000059604645, 0.8500000238418579, 0.6499999761581421]\n",
      "Episode 150\t Max Reward: -5.650676030199975e-06\t Max State: [0.20000000298023224, 0.30000001192092896, 0.4000000059604645, 0.5, 0.699999988079071, 0.6000000238418579]\n",
      "Episode 151\tReward: tensor([-14.3425])\t State: [0.4000000059604645, 0.05000000074505806, 0.3499999940395355, 0.699999988079071, 0.699999988079071, 0.25]\n",
      "Episode 152\tReward: tensor([-10.2983])\t State: [0.44999998807907104, 0.4000000059604645, 0.550000011920929, 0.5, 0.8500000238418579, 0.5]\n",
      "Episode 153\tReward: tensor([-16.1739])\t State: [0.44999998807907104, 0.44999998807907104, 0.4000000059604645, 0.949999988079071, 0.699999988079071, 0.5]\n",
      "Episode 154\tReward: tensor([-16.8647])\t State: [0.5, 0.6000000238418579, 0.550000011920929, 0.20000000298023224, 0.800000011920929, 0.8999999761581421]\n",
      "Episode 155\tReward: tensor([-4.0122])\t State: [0.30000001192092896, 0.4000000059604645, 0.44999998807907104, 0.6499999761581421, 0.75, 1.0]\n",
      "Episode 156\tReward: tensor([-19.5393])\t State: [0.44999998807907104, 0.5, 0.6000000238418579, 1.0, 0.8999999761581421, 0.8999999761581421]\n",
      "Episode 157\tReward: tensor([-15.0704])\t State: [0.4000000059604645, 0.25, 0.6000000238418579, 1.0, 1.0, 0.3499999940395355]\n",
      "Episode 158\tReward: tensor([-17.3676])\t State: [0.550000011920929, 0.0, 0.6499999761581421, 0.800000011920929, 0.800000011920929, 0.550000011920929]\n",
      "Episode 159\tReward: tensor([-32.0441])\t State: [0.550000011920929, 0.8500000238418579, 0.550000011920929, 0.800000011920929, 1.0, 0.5]\n",
      "Episode 160\tReward: tensor([-14.8475])\t State: [0.4000000059604645, 0.6000000238418579, 0.3499999940395355, 1.0, 0.75, 0.3499999940395355]\n",
      "Episode 160\t Max Reward: -5.650676030199975e-06\t Max State: [0.20000000298023224, 0.30000001192092896, 0.4000000059604645, 0.5, 0.699999988079071, 0.6000000238418579]\n",
      "Episode 161\tReward: tensor([-35.9221])\t State: [0.30000001192092896, 0.550000011920929, 0.949999988079071, 0.6499999761581421, 1.0, 0.6000000238418579]\n",
      "Episode 162\tReward: tensor([-2.2753])\t State: [0.25, 0.05000000074505806, 0.44999998807907104, 0.5, 0.6499999761581421, 0.3499999940395355]\n",
      "Episode 163\tReward: tensor([-2.3899])\t State: [0.0, 0.44999998807907104, 0.5, 0.44999998807907104, 0.699999988079071, 0.4000000059604645]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 164\tReward: tensor([-10.1474])\t State: [0.30000001192092896, 0.6000000238418579, 0.15000000596046448, 0.3499999940395355, 0.6499999761581421, 0.4000000059604645]\n",
      "Episode 165\tReward: tensor([-8.7133])\t State: [0.3499999940395355, 0.5, 0.30000001192092896, 0.5, 0.949999988079071, 0.6499999761581421]\n",
      "Episode 166\tReward: tensor([-3.6708])\t State: [0.0, 0.3499999940395355, 0.550000011920929, 0.30000001192092896, 0.949999988079071, 0.4000000059604645]\n",
      "Episode 167\tReward: tensor([-17.8117])\t State: [0.5, 0.699999988079071, 0.0, 0.30000001192092896, 0.699999988079071, 0.6000000238418579]\n",
      "Episode 168\tReward: tensor([-17.3078])\t State: [0.550000011920929, 0.0, 0.800000011920929, 0.25, 0.8999999761581421, 0.44999998807907104]\n",
      "Episode 169\tReward: tensor([-8.5847])\t State: [0.44999998807907104, 0.20000000298023224, 0.4000000059604645, 0.699999988079071, 0.800000011920929, 0.699999988079071]\n",
      "Episode 170\tReward: tensor([-6.6337])\t State: [0.10000000149011612, 0.0, 0.20000000298023224, 0.44999998807907104, 0.75, 0.699999988079071]\n",
      "Episode 170\t Max Reward: -5.650676030199975e-06\t Max State: [0.20000000298023224, 0.30000001192092896, 0.4000000059604645, 0.5, 0.699999988079071, 0.6000000238418579]\n",
      "Episode 171\tReward: tensor([-13.0698])\t State: [0.75, 0.20000000298023224, 0.05000000074505806, 0.4000000059604645, 0.6499999761581421, 0.5]\n",
      "Episode 172\tReward: tensor([-3.8883])\t State: [0.05000000074505806, 0.44999998807907104, 0.6000000238418579, 0.4000000059604645, 0.8500000238418579, 0.30000001192092896]\n",
      "Episode 173\tReward: tensor([-4.6412])\t State: [0.0, 0.25, 0.6499999761581421, 0.3499999940395355, 0.8500000238418579, 0.6499999761581421]\n",
      "Episode 174\tReward: tensor([-4.5062])\t State: [0.0, 0.15000000596046448, 0.4000000059604645, 0.6499999761581421, 1.0, 0.30000001192092896]\n",
      "Episode 175\tReward: tensor([-21.1918])\t State: [0.20000000298023224, 1.0, 0.15000000596046448, 1.0, 0.8999999761581421, 0.6000000238418579]\n",
      "Episode 176\tReward: tensor([-21.2807])\t State: [0.6000000238418579, 0.5, 0.20000000298023224, 0.30000001192092896, 0.699999988079071, 0.550000011920929]\n",
      "Episode 177\tReward: tensor([-8.8850])\t State: [0.44999998807907104, 0.3499999940395355, 0.4000000059604645, 0.550000011920929, 0.800000011920929, 0.5]\n",
      "Episode 178\tReward: tensor([-7.1504])\t State: [0.0, 0.20000000298023224, 0.25, 0.949999988079071, 0.8999999761581421, 0.6499999761581421]\n",
      "Episode 179\tReward: tensor([-28.4167])\t State: [0.5, 0.0, 0.30000001192092896, 0.44999998807907104, 0.6000000238418579, 0.4000000059604645]\n",
      "Episode 180\tReward: tensor([-2.5472])\t State: [0.0, 0.4000000059604645, 0.20000000298023224, 0.5, 0.6499999761581421, 0.949999988079071]\n",
      "Episode 180\t Max Reward: -5.650676030199975e-06\t Max State: [0.20000000298023224, 0.30000001192092896, 0.4000000059604645, 0.5, 0.699999988079071, 0.6000000238418579]\n",
      "Episode 181\tReward: tensor([-0.7880])\t State: [0.10000000149011612, 0.3499999940395355, 0.3499999940395355, 0.5, 0.699999988079071, 0.550000011920929]\n",
      "Episode 182\tReward: tensor([-26.3212])\t State: [0.30000001192092896, 0.6499999761581421, 0.20000000298023224, 0.5, 0.6000000238418579, 0.4000000059604645]\n",
      "Episode 183\tReward: tensor([-364.1309])\t State: [0.25, 0.4000000059604645, 0.25, 0.949999988079071, 0.75, 0.15000000596046448]\n",
      "Episode 184\tReward: tensor([-18.4880])\t State: [0.5, 0.30000001192092896, 0.5, 1.0, 0.75, 0.3499999940395355]\n",
      "Episode 185\tReward: tensor([-3.2442])\t State: [0.0, 0.10000000149011612, 0.5, 0.6499999761581421, 0.8999999761581421, 0.699999988079071]\n",
      "Episode 186\tReward: tensor([-6.1291])\t State: [0.949999988079071, 0.4000000059604645, 0.10000000149011612, 0.4000000059604645, 0.75, 0.44999998807907104]\n",
      "Episode 187\tReward: tensor([-6.8796])\t State: [0.0, 0.5, 0.05000000074505806, 0.949999988079071, 0.8500000238418579, 0.3499999940395355]\n",
      "Episode 188\tReward: tensor([-3.4857])\t State: [0.10000000149011612, 0.3499999940395355, 0.550000011920929, 0.5, 1.0, 0.550000011920929]\n",
      "Episode 189\tReward: tensor([-3.1135])\t State: [0.0, 0.3499999940395355, 0.44999998807907104, 0.5, 1.0, 0.8999999761581421]\n",
      "Episode 190\tReward: tensor([-3.4687])\t State: [0.0, 0.0, 0.5, 0.6000000238418579, 0.8999999761581421, 0.25]\n",
      "Episode 190\t Max Reward: -5.650676030199975e-06\t Max State: [0.20000000298023224, 0.30000001192092896, 0.4000000059604645, 0.5, 0.699999988079071, 0.6000000238418579]\n",
      "Episode 191\tReward: tensor([-67.1250])\t State: [0.4000000059604645, 0.3499999940395355, 0.05000000074505806, 0.3499999940395355, 0.5, 0.6000000238418579]\n",
      "Episode 192\tReward: tensor([-1.1995])\t State: [0.15000000596046448, 0.25, 0.44999998807907104, 0.6000000238418579, 0.699999988079071, 0.699999988079071]\n",
      "Episode 193\tReward: tensor([-4.3197])\t State: [0.20000000298023224, 0.05000000074505806, 0.3499999940395355, 0.550000011920929, 0.6000000238418579, 0.4000000059604645]\n",
      "Episode 194\tReward: tensor([-6.2079])\t State: [0.30000001192092896, 0.25, 0.20000000298023224, 0.4000000059604645, 0.6000000238418579, 0.6000000238418579]\n",
      "Episode 195\tReward: tensor([-2.7711])\t State: [0.0, 0.3499999940395355, 0.6000000238418579, 0.5, 0.75, 0.75]\n",
      "Episode 196\tReward: tensor([-9.4057])\t State: [0.44999998807907104, 0.15000000596046448, 0.4000000059604645, 0.30000001192092896, 0.699999988079071, 0.8500000238418579]\n",
      "Episode 197\tReward: tensor([-7.5154])\t State: [0.4000000059604645, 0.25, 0.4000000059604645, 0.25, 0.699999988079071, 0.8500000238418579]\n",
      "Episode 198\tReward: tensor([-3.9901])\t State: [0.15000000596046448, 0.10000000149011612, 0.3499999940395355, 0.44999998807907104, 0.6000000238418579, 0.6000000238418579]\n",
      "Episode 199\tReward: tensor([-5.0681])\t State: [0.05000000074505806, 0.10000000149011612, 0.550000011920929, 0.699999988079071, 0.699999988079071, 0.6000000238418579]\n",
      "Episode 200\tReward: tensor([-3.0352])\t State: [0.0, 0.20000000298023224, 0.4000000059604645, 0.5, 0.800000011920929, 0.25]\n",
      "Episode 200\t Max Reward: -5.650676030199975e-06\t Max State: [0.20000000298023224, 0.30000001192092896, 0.4000000059604645, 0.5, 0.699999988079071, 0.6000000238418579]\n",
      "Episode 201\tReward: tensor([-11.5333])\t State: [0.5, 0.10000000149011612, 0.44999998807907104, 0.5, 0.699999988079071, 0.5]\n",
      "Episode 202\tReward: tensor([-3.2942])\t State: [0.0, 0.4000000059604645, 0.5, 0.5, 1.0, 0.8999999761581421]\n",
      "Episode 203\tReward: tensor([-8.9648])\t State: [0.3499999940395355, 0.0, 0.6499999761581421, 0.6499999761581421, 0.8500000238418579, 0.25]\n",
      "Episode 204\tReward: tensor([-0.8224])\t State: [0.25, 0.10000000149011612, 0.3499999940395355, 0.44999998807907104, 0.699999988079071, 1.0]\n",
      "Episode 205\tReward: tensor([-7.3595])\t State: [0.4000000059604645, 0.15000000596046448, 0.6000000238418579, 0.4000000059604645, 0.800000011920929, 1.0]\n",
      "Episode 206\tReward: tensor([-17.1356])\t State: [0.550000011920929, 0.15000000596046448, 0.800000011920929, 0.4000000059604645, 0.8999999761581421, 0.8500000238418579]\n",
      "Episode 207\tReward: tensor([-5.4030])\t State: [0.30000001192092896, 0.44999998807907104, 0.5, 0.25, 0.75, 1.0]\n",
      "Episode 208\tReward: tensor([-10.4497])\t State: [0.0, 0.05000000074505806, 0.44999998807907104, 0.550000011920929, 0.6000000238418579, 0.3499999940395355]\n",
      "Episode 209\tReward: tensor([-7.4267])\t State: [0.30000001192092896, 0.3499999940395355, 0.44999998807907104, 1.0, 0.949999988079071, 0.6000000238418579]\n",
      "Episode 210\tReward: tensor([-62.2526])\t State: [0.44999998807907104, 0.0, 0.20000000298023224, 0.550000011920929, 0.550000011920929, 0.4000000059604645]\n",
      "Episode 210\t Max Reward: -5.650676030199975e-06\t Max State: [0.20000000298023224, 0.30000001192092896, 0.4000000059604645, 0.5, 0.699999988079071, 0.6000000238418579]\n",
      "Episode 211\tReward: tensor([-6.8968])\t State: [0.15000000596046448, 0.3499999940395355, 0.20000000298023224, 0.550000011920929, 0.6000000238418579, 0.30000001192092896]\n",
      "Episode 212\tReward: tensor([-7.8174])\t State: [0.0, 0.5, 0.25, 0.4000000059604645, 0.6000000238418579, 0.4000000059604645]\n",
      "Episode 213\tReward: tensor([-419.6948])\t State: [0.44999998807907104, 0.550000011920929, 0.5, 0.0, 0.550000011920929, 0.0]\n",
      "Episode 214\tReward: tensor([-692.2668])\t State: [0.550000011920929, 0.5, 0.25, 0.30000001192092896, 1.0, 0.0]\n",
      "Episode 215\tReward: tensor([-19.7549])\t State: [0.550000011920929, 0.10000000149011612, 0.75, 0.75, 0.800000011920929, 0.8999999761581421]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 216\tReward: tensor([-5.9115])\t State: [0.0, 0.44999998807907104, 0.30000001192092896, 0.550000011920929, 0.8500000238418579, 0.20000000298023224]\n",
      "Episode 217\tReward: tensor([-10.4328])\t State: [0.15000000596046448, 0.20000000298023224, 0.10000000149011612, 0.10000000149011612, 0.5, 0.30000001192092896]\n",
      "Episode 218\tReward: tensor([-3.9747])\t State: [0.0, 0.05000000074505806, 0.6000000238418579, 0.4000000059604645, 0.699999988079071, 0.4000000059604645]\n",
      "Episode 219\tReward: tensor([-6.5015])\t State: [0.25, 0.6499999761581421, 0.0, 0.6000000238418579, 0.699999988079071, 0.3499999940395355]\n",
      "Episode 220\tReward: tensor([-5.7183])\t State: [0.0, 0.550000011920929, 0.30000001192092896, 0.550000011920929, 0.6499999761581421, 0.30000001192092896]\n",
      "Episode 220\t Max Reward: -5.650676030199975e-06\t Max State: [0.20000000298023224, 0.30000001192092896, 0.4000000059604645, 0.5, 0.699999988079071, 0.6000000238418579]\n",
      "Episode 221\tReward: tensor([-22.2574])\t State: [0.3499999940395355, 0.44999998807907104, 0.4000000059604645, 0.30000001192092896, 0.6000000238418579, 0.44999998807907104]\n",
      "Episode 222\tReward: tensor([-4.1442])\t State: [0.0, 0.44999998807907104, 0.4000000059604645, 0.05000000074505806, 0.6499999761581421, 0.25]\n",
      "Episode 223\tReward: tensor([-9.5992])\t State: [0.4000000059604645, 0.44999998807907104, 0.30000001192092896, 0.30000001192092896, 0.699999988079071, 0.8999999761581421]\n",
      "Episode 224\tReward: tensor([-10.5244])\t State: [0.4000000059604645, 0.6000000238418579, 0.4000000059604645, 0.5, 0.800000011920929, 0.550000011920929]\n",
      "Episode 225\tReward: tensor([-26.2690])\t State: [0.0, 0.5, 0.3499999940395355, 1.0, 0.6000000238418579, 0.44999998807907104]\n",
      "Episode 226\tReward: tensor([-7.6122])\t State: [0.4000000059604645, 0.05000000074505806, 0.5, 0.6000000238418579, 0.699999988079071, 0.949999988079071]\n",
      "Episode 227\tReward: tensor([-3.8665])\t State: [0.0, 0.05000000074505806, 0.5, 0.3499999940395355, 0.699999988079071, 0.20000000298023224]\n",
      "Episode 228\tReward: tensor([-9.8628])\t State: [0.15000000596046448, 0.25, 0.10000000149011612, 0.699999988079071, 0.800000011920929, 0.25]\n",
      "Episode 229\tReward: tensor([-10.2745])\t State: [0.25, 0.6000000238418579, 0.3499999940395355, 0.05000000074505806, 0.6499999761581421, 0.44999998807907104]\n",
      "Episode 230\tReward: tensor([-25.8349])\t State: [0.25, 0.699999988079071, 0.0, 0.44999998807907104, 0.6499999761581421, 0.20000000298023224]\n",
      "Episode 230\t Max Reward: -5.650676030199975e-06\t Max State: [0.20000000298023224, 0.30000001192092896, 0.4000000059604645, 0.5, 0.699999988079071, 0.6000000238418579]\n",
      "Episode 231\tReward: tensor([-26.1626])\t State: [0.6000000238418579, 0.05000000074505806, 0.5, 0.550000011920929, 0.6499999761581421, 0.3499999940395355]\n",
      "Episode 232\tReward: tensor([-54.2228])\t State: [0.4000000059604645, 0.699999988079071, 0.44999998807907104, 0.10000000149011612, 0.6000000238418579, 0.30000001192092896]\n",
      "Episode 233\tReward: tensor([-1.0507])\t State: [0.25, 0.0, 0.4000000059604645, 0.5, 0.75, 0.30000001192092896]\n",
      "Episode 234\tReward: tensor([-32.6582])\t State: [0.5, 0.10000000149011612, 0.44999998807907104, 0.0, 0.6000000238418579, 0.3499999940395355]\n",
      "Episode 235\tReward: tensor([-5.9062])\t State: [0.3499999940395355, 0.20000000298023224, 0.3499999940395355, 0.6000000238418579, 0.6499999761581421, 0.5]\n",
      "Episode 236\tReward: tensor([-21.6884])\t State: [0.30000001192092896, 0.550000011920929, 0.3499999940395355, 0.30000001192092896, 1.0, 0.10000000149011612]\n",
      "Episode 237\tReward: tensor([-8.2268])\t State: [0.20000000298023224, 0.6499999761581421, 0.0, 0.5, 0.6499999761581421, 0.550000011920929]\n",
      "Episode 238\tReward: tensor([-62.1093])\t State: [0.550000011920929, 0.699999988079071, 0.30000001192092896, 0.4000000059604645, 0.6000000238418579, 0.5]\n",
      "Episode 239\tReward: tensor([-8.5045])\t State: [0.25, 0.550000011920929, 0.6000000238418579, 0.6499999761581421, 0.75, 0.949999988079071]\n",
      "Episode 240\tReward: tensor([-7.7229])\t State: [0.05000000074505806, 0.5, 0.15000000596046448, 0.25, 0.6000000238418579, 0.550000011920929]\n",
      "Episode 240\t Max Reward: -5.650676030199975e-06\t Max State: [0.20000000298023224, 0.30000001192092896, 0.4000000059604645, 0.5, 0.699999988079071, 0.6000000238418579]\n",
      "Episode 241\tReward: tensor([-2.7136])\t State: [0.0, 0.05000000074505806, 0.550000011920929, 0.5, 0.699999988079071, 0.4000000059604645]\n",
      "Episode 242\tReward: tensor([-12.5018])\t State: [0.05000000074505806, 0.6000000238418579, 0.15000000596046448, 0.550000011920929, 0.6000000238418579, 0.44999998807907104]\n",
      "Episode 243\tReward: tensor([-20.1949])\t State: [0.699999988079071, 0.0, 0.5, 0.44999998807907104, 0.6499999761581421, 0.550000011920929]\n",
      "Episode 244\tReward: tensor([-10.3963])\t State: [0.800000011920929, 0.25, 0.25, 0.5, 0.6499999761581421, 0.6000000238418579]\n",
      "Episode 245\tReward: tensor([-10.9579])\t State: [0.5, 0.30000001192092896, 0.3499999940395355, 0.44999998807907104, 0.75, 0.5]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-1893facb6dc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Select and perform an action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxreward\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/fluid-parameter-learning/FluidEnv.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0msuccess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecute_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'change-param.js'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"change-param.js ran incorrectly\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/urop/lib/python3.8/site-packages/Naked/toolshed/shell.py\u001b[0m in \u001b[0;36mexecute_js\u001b[0;34m(file_path, arguments)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mjs_command\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'node '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjs_command\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# return result of execute() of node.js file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mDEBUG_FLAG\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/urop/lib/python3.8/site-packages/Naked/toolshed/shell.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(command)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/urop/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Including KeyboardInterrupt, wait handled that.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/urop/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1077\u001b[0m             \u001b[0mendtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m             \u001b[0;31m# https://bugs.python.org/issue25942\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/urop/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36m_wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1802\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1803\u001b[0m                             \u001b[0;32mbreak\u001b[0m  \u001b[0;31m# Another thread waited.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1804\u001b[0;31m                         \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1805\u001b[0m                         \u001b[0;31m# Check the pid and loop as waitpid has been known to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1806\u001b[0m                         \u001b[0;31m# return 0 even without WNOHANG in odd situations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/urop/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36m_try_wait\u001b[0;34m(self, wait_flags)\u001b[0m\n\u001b[1;32m   1760\u001b[0m             \u001b[0;34m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1762\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait_flags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mChildProcessError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m                 \u001b[0;31m# This happens if SIGCLD is set to be ignored or waiting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "importlib.reload(FluidEnv)\n",
    "\n",
    "env = FluidEnv.FluidEnv(num_params, '', 50)\n",
    "\n",
    "maxreward = -100000\n",
    "maxstate = []\n",
    "num_episodes = 500\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    t = 0\n",
    "    while not done:\n",
    "        # Select and perform an action\n",
    "        action = select_action(state)\n",
    "        next_state, reward, done, _ = env.step(torch.Tensor(actions[action.item()]))\n",
    "        \n",
    "        if reward > maxreward:\n",
    "            maxreward = reward\n",
    "            maxstate = next_state\n",
    "        \n",
    "        reward = torch.tensor([reward], device=device)        \n",
    "        # Store the transition in memory\n",
    "        memory.push(state.unsqueeze(0), action, next_state.unsqueeze(0), reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the target network)\n",
    "        optimize_model()\n",
    "        t += 1\n",
    "    # Update the target network, copying all weights and biases in DQN\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    if i_episode % 1 == 0:\n",
    "        print('Episode {}\\tReward: {}\\t State: {}'.format(i_episode, reward, state.tolist()))\n",
    "\n",
    "    if i_episode % 10 == 0:\n",
    "        print('Episode {}\\t Max Reward: {}\\t Max State: {}'.format(i_episode, maxreward, maxstate.tolist()))\n",
    "\n",
    "print('Complete', maxreward, maxstate.tolist())\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.05, 0, 0, 0, 0, 0],\n",
       " [0, 0.05, 0, 0, 0, 0],\n",
       " [0, 0, 0.05, 0, 0, 0],\n",
       " [0, 0, 0, 0.05, 0, 0],\n",
       " [0, 0, 0, 0, 0.05, 0],\n",
       " [0, 0, 0, 0, 0, 0.05],\n",
       " [-0.05, 0, 0, 0, 0, 0],\n",
       " [0, -0.05, 0, 0, 0, 0],\n",
       " [0, 0, -0.05, 0, 0, 0],\n",
       " [0, 0, 0, -0.05, 0, 0],\n",
       " [0, 0, 0, 0, -0.05, 0],\n",
       " [0, 0, 0, 0, 0, -0.05]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
